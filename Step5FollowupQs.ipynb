{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Q's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as sm\n",
    "#from sklearn import tree\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score,mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from pathlib import Path\n",
    "from DataCleanUp import *\n",
    "\n",
    "from model import modelit\n",
    "\n",
    "M = modelit()\n",
    "M.predict(ret=False)\n",
    "print(\"Score from Random Forest = \",M.score_v)\n",
    "#M.plotYvsPredit() # Does not work in Jupyter. so c below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(M.yP, M.pred, edgecolors=(0, 0, 0))\n",
    "ax.plot([M.yP.min(), M.yP.max()], [M.yP.min(), M.yP.max()], 'k--', lw=4)\n",
    "ax.set_xlabel('Observed')\n",
    "ax.set_ylabel('Predicted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's the most important feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.df_train.columns)\n",
    "result = sm.ols(formula=\"USD_hamP ~ USD_Hest \", data=M.df_train).fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### as expected estimate (high) just dominates the field with Adj R2 of 0.928 (even I include NaNs Adj R2 is still above 0.7), but including this term will introduce multicollinearity prb with others, as this is pretty much what an experts opinion after all data is considered. Plus, not all of work has this number available. Definitely model with it, but domination should b considered with a grain of salt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sm.ols(formula=\"USD_DjiP ~ artist_name + auct_yr + measure_hwd_cm + unique +artist_birth_year + exec_pmortem + category\", data=M.df_train.fillna(-999)).fit()\n",
    "print(result.summary2())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hammer_price is not a good item to try to predict. We have data going back to 1985. Asset price has risen considerably since. So, if keep on using the USD equivalent, then we will miss all signals. I have imported DJI for this reason to find relative price to stock market. With estimate P, it's dominance & equivalent pricing given the date, would make any model seem like a good model, but remove that its unlikely any model will be able to go beyond asset inflation bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.df_train.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = modelit()\n",
    "M.predict(ret=False, colnms='measure_hwd_cm,category')\n",
    "print(\"Score from Random Forest = \",M.score_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### artist name + measurement + category didn't make interesting model either linearly or with Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating uncorrleated Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I usually do this by few steps.\n",
    "#### This is standard Modus Operandi of any Prop Trading desk or Hedge Fund\n",
    "1st create returns based on few features.\n",
    "Category\n",
    "Artist\n",
    "Genre\n",
    "country\n",
    "etc...\n",
    "#### create return series & look for correlation coeff & more opposite or close to 0 as possible the better.\n",
    "### Use an optimizer (I always used commercial optimizer so could not do this in Python as this requires higher math than simplex) to create bounds, such as no Genre, country, or artist may take more than X% (X=5~14%) of portfolio & consider rebalance whenever the threshold is reached.\n",
    "\n",
    "## Recommendation category really depends on the correlation matrix one has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = transform_data(pd.read_csv('data.csv', encoding='latin-1'))\n",
    "colnm = 'USD_hamP,USD_DjiP,auct_yr,measure_hwd_cm,unique,exec_pmortem,category'.split(',')\n",
    "sol = da.query(\"artist_name =='Sol LeWitt'\")[colnm].sort_values(by=['category', 'auct_yr'])\n",
    "sol = sol[sol.auct_yr > 2000]\n",
    "g = sol.groupby(['category','auct_yr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol_CmeanCnt = sol[sol.auct_yr > 2000].groupby(['category','auct_yr']).USD_DjiP.agg(['mean', 'count'])\n",
    "plt.plot(sol_AmeanCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### according to above price never did recover"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As for confidence intervals,\n",
    "Not sure why forestci is not working. Likely overflow = Data issue due to NaN's or other large data issues\n",
    "Will need to investigate further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"/opt/miniconda3/envs/SPk/lib/python3.6/site-packages/forestci/calibration.py:86: RuntimeWarning: overflow encountered in exp\n",
    "  g_eta_raw = np.exp(np.dot(XX, eta)) * mask\n",
    "/opt/miniconda3/envs/SPk/lib/python3.6/site-packages/forestci/calibration.py:101: RuntimeWarning: overflow encountered in exp\n",
    "  g_eta_raw = np.exp(np.dot(XX, eta_hat)) * mask\n",
    "/opt/miniconda3/envs/SPk/lib/python3.6/site-packages/forestci/calibration.py:102: RuntimeWarning: invalid value encountered in true_divide\n",
    "  g_eta_main = g_eta_raw / sum(g_eta_raw)\n",
    "  \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The idea here is that as with Ensemble models, number of tree (30 in this case) predicting each Y. So, one can make variance & mean based on this, which will be much more accurate than comparing simple variance of entire Y predicted vs actual.\n",
    "Unfortunately, the code I am using is failing due to overflow, which means I will have to debug line by line or just create my own. Not 2~3 hr job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SPky3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
